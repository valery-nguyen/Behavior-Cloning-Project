{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File structure\n",
    "Folder and files in main directory:\n",
    "- model.py\n",
    "- Behavior-Cloning.ipynb\n",
    "- model.json\n",
    "- model.h5\n",
    "- drive.py\n",
    "- driving_log.csv\n",
    "- 'IMG' folder\n",
    "    -- 'All .jpg images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Dataset: 333MB [01:07, 4.95MB/s]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get training dataset from Udacity website\n",
    "\n",
    "# Imports\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('driving_log1.csv'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Train Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://d17h27t6h515a5.cloudfront.net/topher/2016/December/584f6edd_data/data.zip',\n",
    "            'data.zip',\n",
    "            pbar.hook)\n",
    "\n",
    "    # Unzip data set\n",
    "    zip_ref = zipfile.ZipFile('data.zip', 'r')\n",
    "    zip_ref.extractall('')\n",
    "    zip_ref.close()\n",
    "\n",
    "    # Move dataset to folder location\n",
    "    shutil.move('data/IMG/','IMG')\n",
    "    shutil.move('data/driving_log.csv','driving_log.csv')\n",
    "    \n",
    "    print('Training data downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules loaded.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Fix error with TF and Keras\n",
    "import tensorflow as tf\n",
    "tf.python.control_flow_ops = tf\n",
    "\n",
    "print('Modules loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the images\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import os.path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "img_path = os.getcwd()+'/IMG'\n",
    "imgs = np.empty((0,160,320,3))\n",
    "valid_ext = [\".jpg\"]\n",
    "\n",
    "for f in os.listdir(img_path):\n",
    "    f_ext = os.path.splitext(f)[1]\n",
    "    if f_ext.lower() not in valid_ext:\n",
    "        continue\n",
    "    img = np.squeeze(Image.open(os.path.join(img_path,f)))\n",
    "    imgs = np.append(imgs, np.expand_dims(img, axis=0),axis=0)\n",
    "    imgs = imgs.astype(np.uint8)\n",
    "\n",
    "print('Images loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the driving log data\n",
    "\n",
    "# Imports\n",
    "import csv, sys\n",
    "\n",
    "filename = 'driving_log.csv'\n",
    "angles = []\n",
    "imgs_fname = []\n",
    "\n",
    "with open(filename, 'rt') as csvfile:\n",
    "    \n",
    "    # Load driving angle data\n",
    "    anglereader = csv.reader(csvfile, delimiter=',')\n",
    "    \n",
    "    try:\n",
    "        for row in anglereader:\n",
    "            angles.append(row[-1])\n",
    "          \n",
    "            fln = row[0].split('/')\n",
    "            imgs_fname.append(fln[-1])\n",
    "            \n",
    "    except csv.Error as e:\n",
    "        sys.exit('file %s, line %d: %s' % (filename, reader.line_num, e)) \n",
    "    \n",
    "    print('Driving driving log data loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verify that the images and driving data are loaded correctly\n",
    "# Display random sample image and matching driving data\n",
    "\n",
    "# Imports\n",
    "import random\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "index = random.randint(0, len(imgs)-1)\n",
    "image = imgs[index]\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.imshow(image)\n",
    "\n",
    "print('The image dimensions are: ',image.shape)\n",
    "print('The image filename is: ',imgs_fname[index])\n",
    "print('The driving angle is: ',angles[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up train data\n",
    "X_train, y_train = imgs, angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Summary of Dataset information\n",
    "\n",
    "# Number of training examples\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "# Shape of an traffic sign image\n",
    "image_shape = X_train[0].shape\n",
    "\n",
    "# Number of unique classes/labels there are in the dataset\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "# Display information\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing functions for image set\n",
    "\n",
    "# Imports\n",
    "import cv2\n",
    "\n",
    "def preprocess_set(img):\n",
    "    \n",
    "    img_gr = grayscale_set(img)\n",
    "    img_nm = normalize_set(img_gr)\n",
    "    img_p = np.expand_dims(img_nm, axis=3)\n",
    "    \n",
    "    return img_p\n",
    "    \n",
    "# Grayscale image set\n",
    "def grayscale_set(img):\n",
    "    img_gr = []\n",
    "    \n",
    "    for i in range(0,len(img)):\n",
    "        img_gr.append(cv2.cvtColor(img[i],cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "    return img_gr\n",
    "\n",
    "# Normalize image set (values between 0-1)\n",
    "def normalize_set(img):\n",
    "    grayscale_max = 255\n",
    "    img_nm = []\n",
    "    \n",
    "    for i in range(0,len(img)):\n",
    "        img_nm.append(img[i]/grayscale_max)\n",
    "    \n",
    "    return img_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shuffle training image set\n",
    "\n",
    "# Imports\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocess training and validation image sets\n",
    "X_train = preprocess_set(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One Hot encode the labels to the variable y_one_hot\n",
    "\n",
    "# Imports\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_train_one_hot = label_binarizer.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up train and validation data\n",
    "\n",
    "# Imports\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Split data into train and validation data\n",
    "X_train, X_validation, y_train_one_hot, y_validation_one_hot = train_test_split(X_train, y_train_one_hot, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CHOOSE TO IMPORT MODEL AND WEIGHTS\n",
    "# OPTIONAL: import model.json and model.h5\n",
    "\n",
    "# Imports\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# Load the model from model.json\n",
    "json_data=open('model.json').read()\n",
    "json_string = json.loads(json_data)\n",
    "model = model_from_json(json_string)\n",
    "\n",
    "#Load the weights from model.h5\n",
    "model.load_weights('model.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CHOOSE TO SET UP NEW MODEL AND WEIGHTS\n",
    "\n",
    "# Network model\n",
    "\n",
    "# Imports\n",
    "from keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "# Parameters\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(160, 320, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(n_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define train batch generator\n",
    "\n",
    "# Imports\n",
    "import math\n",
    "\n",
    "# Parameters\n",
    "batch_size = 100\n",
    "\n",
    "def generate_batch(X_train, y_train_one_hot, batch_size):\n",
    "    \n",
    "    batch_max = math.floor(len(X_train)/batch_size)\n",
    "    \n",
    "    while 1:\n",
    "        for batch_num in range (batch_max):\n",
    "            if batch_num > batch_max:\n",
    "                return\n",
    "            X_train_batch = X_train[batch_num*batch_size:(batch_num+1)*batch_size]\n",
    "            y_train_one_hot_batch = y_train_one_hot[batch_num*batch_size:(batch_num+1)*batch_size]\n",
    "            yield X_train_batch, y_train_one_hot_batch \n",
    "\n",
    "# Compile and train the mod\n",
    "model.compile('adam', 'categorical_crossentropy', ['accuracy'])\n",
    "\n",
    "history = model.fit_generator(generate_batch(X_train, y_train_one_hot, batch_size),\n",
    "                              samples_per_epoch = len(X_train), nb_epoch = 10, \n",
    "                              validation_data = (X_validation, y_validation_one_hot), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the model and weights\n",
    "\n",
    "# Imports\n",
    "from keras.models import model_from_json\n",
    "import json\n",
    "\n",
    "# Save the model to model.json\n",
    "json_string = model.to_json()\n",
    "with open('model.json', 'w') as f:\n",
    "     json.dump(json_string, f)\n",
    "\n",
    "# Save the weights to model.h5\n",
    "model.save_weights('model.h5')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
